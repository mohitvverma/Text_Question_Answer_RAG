{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-10T12:54:34.586510Z",
     "start_time": "2024-08-10T12:54:34.581437Z"
    }
   },
   "source": "import os",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:54:34.980464Z",
     "start_time": "2024-08-10T12:54:34.941078Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain import HuggingFaceHub\n",
    "from langchain_huggingface.llms import HuggingFaceEndpoint, HuggingFacePipeline"
   ],
   "id": "1c6e20732bd4a190",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:54:35.044441Z",
     "start_time": "2024-08-10T12:54:35.029227Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ],
   "id": "1197838786b161bc",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:54:35.672974Z",
     "start_time": "2024-08-10T12:54:35.112109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.schema import Document\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "\n",
    "# Text splitter\n",
    "from langchain.text_splitter import TextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain.storage import InMemoryStore\n",
    "from langchain.document_loaders import TextLoader"
   ],
   "id": "219e62fbb7a0e166",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:54:35.695172Z",
     "start_time": "2024-08-10T12:54:35.675781Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain.embeddings import LocalAIEmbeddings, HuggingFaceBgeEmbeddings",
   "id": "c7c4d8d21cad568c",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:02.617197Z",
     "start_time": "2024-08-10T12:54:35.697067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "model_name = \"BAAI/bge-small-en-v1.5\"\n",
    "encode_kwargs = {'normalize_embeddings': True} # set True to compute cosine similarity\n",
    "\n",
    "bge_embeddings = HuggingFaceBgeEmbeddings(\n",
    "    model_name=model_name,\n",
    "    model_kwargs={'device': 'mps'},\n",
    "    encode_kwargs=encode_kwargs\n",
    ")"
   ],
   "id": "84ae0b73c5e33b34",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:02.636528Z",
     "start_time": "2024-08-10T12:55:02.622583Z"
    }
   },
   "cell_type": "code",
   "source": [
    "text = \"\"\"\n",
    "Agent: This is Martin with Goodyear. \n",
    "customer: Hello. Is this Martin? Yeah. Hey, Martin. How are you doing? This is Dwayne Mann. \n",
    "Agent: Hey, Dwayne. \n",
    "customer: Hey. Yeah. The tires were here today. \n",
    "customer: Is there any update on \n",
    "customer: the \n",
    "customer: because I haven't received the email about the the installation fee being waived and the return of the \n",
    "customer: old mobile fee. \n",
    "Agent: Oh, gotcha. So you already have those installed? \n",
    "customer: Say that again, sir. \n",
    "Agent: You already have those installed? \n",
    "customer: Where? \n",
    "Agent: No. I'm saying you you already have those installed? \n",
    "customer: No. They they're in the process of doing it now. \n",
    "Agent: Oh, gotcha. Okay. So, yeah, we'll we'll go ahead and get that \n",
    "Agent: for the \n",
    "Agent: installation and the mobile fee to be refunded. We'll go ahead and get the paperwork that we need from the installer. We can go ahead and get that taken care of for you. \n",
    "customer: Oh, so that's never was that that was never submitted yet. Right? \n",
    "Agent: No. That's only when you do the installation. There's, like, a paper invoice that the store gives you, but we'll get it from the store specifically. \n",
    "customer: Oh, okay. Yeah. They're in the process of doing it now. Oh, so you can't put in for the the waive the fee until the installment is done? \n",
    "Agent: Yeah. So the the way that the reimbursement department has to do it is that once the tire is installed, they just need to see the proof being the paperwork, and then they go ahead and just process it. \n",
    "customer: Oh, okay. Okay. \n",
    "customer: Okay. So yeah. Because I was wondering, you know, I just wanted to know what what was my current balance, you know, because \n",
    "customer: Yeah. \n",
    "Agent: So that So \n",
    "customer: that we have the So that one nineteen \n",
    "customer: is gonna be subtracted from the what the what the initial charge was originally supposed to be. Correct? \n",
    "Agent: Yep. \n",
    "customer: Which was one o the company \n",
    "Agent: that we went over the charges? Right. I got it written I \n",
    "customer: got it written down, which was nine zero six eleven, I think it was. Nine zero six eleven. \n",
    "Agent: Yeah. And then you get the, \n",
    "Agent: I think, the two hundred dollar rebate? \n",
    "Agent: Yeah. I don't know if you yeah. I don't know \n",
    "customer: if you still found out. The rebate was two twenty five. Two hundred Yeah. \n",
    "Agent: I don't know if you, Yeah. I don't know if you still ask the bank if if they can let you do that the way that the bill comes out less. \n",
    "customer: Yeah. The bank said that it it still read nine thirty six. \n",
    "Agent: Oh, okay. Perfect. So, yeah, you you can still use the rebate card towards the \n",
    "Agent: the balance, I believe. Yeah. Like I had mentioned to you. \n",
    "customer: Yeah. They said it still read nine thirty six. But once once you submit the paperwork, that will be reduced at the bank as well. Correct? \n",
    "Agent: Yeah. \n",
    "customer: Okay. The bank the bank goes ahead and \n",
    "Agent: receives the credit back with the one nineteen eighty and then the twenty dollars. \n",
    "customer: Okay. I greatly appreciate. So once every once those refunds are made, then you'll send me an email stating such? \n",
    "Agent: Yeah. Should I get you an email? \n",
    "customer: Okay. Not that. \n",
    "Agent: If you want, give me a callback once the tires are installed so I can just have the installer send me the paperwork. \n",
    "customer: Say that again, sir? \n",
    "Agent: If you want, once the tires are finished being installed, give me a callback and let me know so that I can just reach out to the store and then have them send me the paperwork for it. \n",
    "customer: Okay. Wonderful. I greatly appreciate \n",
    "Agent: it. Yeah. Of course. It's definitely my pleasure. \n",
    "customer: Yeah. Thank \n",
    "Agent: you, Paul. Alright. Of course. It's been my pleasure. So, yeah, just reach out to me as soon as you go ahead and get that Mhmm. Completed, and then I'll go I'll go ahead and reach out to them. \n",
    "customer: Okay. Yeah. They put them on that. Alright. Perfect. Alright. Bye bye. Bye bye.\n",
    "\"\"\""
   ],
   "id": "9d7eb9ebb4efdf77",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:02.644073Z",
     "start_time": "2024-08-10T12:55:02.638842Z"
    }
   },
   "cell_type": "code",
   "source": "t1 = TextLoader(\"/Users/mohitverma/Documents/Text_Question_Answer_RAG/text_data/sample_text.txt\")",
   "id": "63a661b0fd3acac7",
   "outputs": [],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:02.655658Z",
     "start_time": "2024-08-10T12:55:02.646938Z"
    }
   },
   "cell_type": "code",
   "source": [
    "docs = t1.load()\n",
    "docs[0]"
   ],
   "id": "72e7756b220f488b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': '/Users/mohitverma/Documents/Text_Question_Answer_RAG/text_data/sample_text.txt'}, page_content='Agent: This is Martin with Goodyear.\\ncustomer: Hello. Is this Martin? Yeah. Hey, Martin. How are you doing? This is Dwayne Mann.\\nAgent: Hey, Dwayne.\\ncustomer: Hey. Yeah. The tires were here today.\\ncustomer: Is there any update on\\ncustomer: the\\ncustomer: because I haven\\'t received the email about the the installation fee being waived and the return of the\\ncustomer: old mobile fee.\\nAgent: Oh, gotcha. So you already have those installed?\\ncustomer: Say that again, sir.\\nAgent: You already have those installed?\\ncustomer: Where?\\nAgent: No. I\\'m saying you you already have those installed?\\ncustomer: No. They they\\'re in the process of doing it now.\\nAgent: Oh, gotcha. Okay. So, yeah, we\\'ll we\\'ll go ahead and get that\\nAgent: for the\\nAgent: installation and the mobile fee to be refunded. We\\'ll go ahead and get the paperwork that we need from the installer. We can go ahead and get that taken care of for you.\\ncustomer: Oh, so that\\'s never was that that was never submitted yet. Right?\\nAgent: No. That\\'s only when you do the installation. There\\'s, like, a paper invoice that the store gives you, but we\\'ll get it from the store specifically.\\ncustomer: Oh, okay. Yeah. They\\'re in the process of doing it now. Oh, so you can\\'t put in for the the waive the fee until the installment is done?\\nAgent: Yeah. So the the way that the reimbursement department has to do it is that once the tire is installed, they just need to see the proof being the paperwork, and then they go ahead and just process it.\\ncustomer: Oh, okay. Okay.\\ncustomer: Okay. So yeah. Because I was wondering, you know, I just wanted to know what what was my current balance, you know, because\\ncustomer: Yeah.\\nAgent: So that So\\ncustomer: that we have the So that one nineteen\\ncustomer: is gonna be subtracted from the what the what the initial charge was originally supposed to be. Correct?\\nAgent: Yep.\\ncustomer: Which was one o the company\\nAgent: that we went over the charges? Right. I got it written I\\ncustomer: got it written down, which was nine zero six eleven, I think it was. Nine zero six eleven.\\nAgent: Yeah. And then you get the,\\nAgent: I think, the two hundred dollar rebate?\\nAgent: Yeah. I don\\'t know if you yeah. I don\\'t know\\ncustomer: if you still found out. The rebate was two twenty five. Two hundred Yeah.\\nAgent: I don\\'t know if you, Yeah. I don\\'t know if you still ask the bank if if they can let you do that the way that the bill comes out less.\\ncustomer: Yeah. The bank said that it it still read nine thirty six.\\nAgent: Oh, okay. Perfect. So, yeah, you you can still use the rebate card towards the\\nAgent: the balance, I believe. Yeah. Like I had mentioned to you.\\ncustomer: Yeah. They said it still read nine thirty six. But once once you submit the paperwork, that will be reduced at the bank as well. Correct?\\nAgent: Yeah.\\ncustomer: Okay. The bank the bank goes ahead and\\nAgent: receives the credit back with the one nineteen eighty and then the twenty dollars.\\ncustomer: Okay. I greatly appreciate. So once every once those refunds are made, then you\\'ll send me an email stating such?\\nAgent: Yeah. Should I get you an email?\\ncustomer: Okay. Not that.\\nAgent: If you want, give me a callback once the tires are installed so I can just have the installer send me the paperwork.\\ncustomer: Say that again, sir?\\nAgent: If you want, once the tires are finished being installed, give me a callback and let me know so that I can just reach out to the store and then have them send me the paperwork for it.\\ncustomer: Okay. Wonderful. I greatly appreciate\\nAgent: it. Yeah. Of course. It\\'s definitely my pleasure.\\ncustomer: Yeah. Thank\\nAgent: you, Paul. Alright. Of course. It\\'s been my pleasure. So, yeah, just reach out to me as soon as you go ahead and get that Mhmm. Completed, and then I\\'ll go I\\'ll go ahead and reach out to them.\\ncustomer: Okay. Yeah. They put them on that. Alright. Perfect. Alright. Bye bye. Bye bye.\\n\"\"\"text = \"\"\"\\nAgent: This is Martin with Goodyear.\\ncustomer: Hello. Is this Martin? Yeah. Hey, Martin. How are you doing? This is Dwayne Mann.\\nAgent: Hey, Dwayne.\\ncustomer: Hey. Yeah. The tires were here today.\\ncustomer: Is there any update on\\ncustomer: the\\ncustomer: because I haven\\'t received the email about the the installation fee being waived and the return of the\\ncustomer: old mobile fee.\\nAgent: Oh, gotcha. So you already have those installed?\\ncustomer: Say that again, sir.\\nAgent: You already have those installed?\\ncustomer: Where?\\nAgent: No. I\\'m saying you you already have those installed?\\ncustomer: No. They they\\'re in the process of doing it now.\\nAgent: Oh, gotcha. Okay. So, yeah, we\\'ll we\\'ll go ahead and get that\\nAgent: for the\\nAgent: installation and the mobile fee to be refunded. We\\'ll go ahead and get the paperwork that we need from the installer. We can go ahead and get that taken care of for you.\\ncustomer: Oh, so that\\'s never was that that was never submitted yet. Right?\\nAgent: No. That\\'s only when you do the installation. There\\'s, like, a paper invoice that the store gives you, but we\\'ll get it from the store specifically.\\ncustomer: Oh, okay. Yeah. They\\'re in the process of doing it now. Oh, so you can\\'t put in for the the waive the fee until the installment is done?\\nAgent: Yeah. So the the way that the reimbursement department has to do it is that once the tire is installed, they just need to see the proof being the paperwork, and then they go ahead and just process it.\\ncustomer: Oh, okay. Okay.\\ncustomer: Okay. So yeah. Because I was wondering, you know, I just wanted to know what what was my current balance, you know, because\\ncustomer: Yeah.\\nAgent: So that So\\ncustomer: that we have the So that one nineteen\\ncustomer: is gonna be subtracted from the what the what the initial charge was originally supposed to be. Correct?\\nAgent: Yep.\\ncustomer: Which was one o the company\\nAgent: that we went over the charges? Right. I got it written I\\ncustomer: got it written down, which was nine zero six eleven, I think it was. Nine zero six eleven.\\nAgent: Yeah. And then you get the,\\nAgent: I think, the two hundred dollar rebate?\\nAgent: Yeah. I don\\'t know if you yeah. I don\\'t know\\ncustomer: if you still found out. The rebate was two twenty five. Two hundred Yeah.\\nAgent: I don\\'t know if you, Yeah. I don\\'t know if you still ask the bank if if they can let you do that the way that the bill comes out less.\\ncustomer: Yeah. The bank said that it it still read nine thirty six.\\nAgent: Oh, okay. Perfect. So, yeah, you you can still use the rebate card towards the\\nAgent: the balance, I believe. Yeah. Like I had mentioned to you.\\ncustomer: Yeah. They said it still read nine thirty six. But once once you submit the paperwork, that will be reduced at the bank as well. Correct?\\nAgent: Yeah.\\ncustomer: Okay. The bank the bank goes ahead and\\nAgent: receives the credit back with the one nineteen eighty and then the twenty dollars.\\ncustomer: Okay. I greatly appreciate. So once every once those refunds are made, then you\\'ll send me an email stating such?\\nAgent: Yeah. Should I get you an email?\\ncustomer: Okay. Not that.\\nAgent: If you want, give me a callback once the tires are installed so I can just have the installer send me the paperwork.\\ncustomer: Say that again, sir?\\nAgent: If you want, once the tires are finished being installed, give me a callback and let me know so that I can just reach out to the store and then have them send me the paperwork for it.\\ncustomer: Okay. Wonderful. I greatly appreciate\\nAgent: it. Yeah. Of course. It\\'s definitely my pleasure.\\ncustomer: Yeah. Thank\\nAgent: you, Paul. Alright. Of course. It\\'s been my pleasure. So, yeah, just reach out to me as soon as you go ahead and get that Mhmm. Completed, and then I\\'ll go I\\'ll go ahead and reach out to them.\\ncustomer: Okay. Yeah. They put them on that. Alright. Perfect. Alright. Bye bye. Bye bye.\\n')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:02.664618Z",
     "start_time": "2024-08-10T12:55:02.658492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "chunk_size = len(docs[0].page_content.split(' ')) // 3\n",
    "chunk_overlap = len(docs[0].page_content.split(' ')) // 7\n",
    "print(chunk_size, chunk_overlap)"
   ],
   "id": "3ace1b53abb18a58",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "438 187\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:03.308624Z",
     "start_time": "2024-08-10T12:55:02.667081Z"
    }
   },
   "cell_type": "code",
   "source": [
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"full_documents\",\n",
    "    embedding_function=bge_embeddings\n",
    ")\n",
    "\n",
    "store=InMemoryStore()"
   ],
   "id": "e76334ca978bd48",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohitverma/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 0.4. An updated version of the class exists in the langchain-chroma package and should be used instead. To use it run `pip install -U langchain-chroma` and import as `from langchain_chroma import Chroma`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:03.313426Z",
     "start_time": "2024-08-10T12:55:03.310272Z"
    }
   },
   "cell_type": "code",
   "source": [
    "full_doc_retriever = ParentDocumentRetriever(\n",
    "    child_splitter=child_splitter,\n",
    "    vectorstore=vector_store,\n",
    "    docstore=store\n",
    ")"
   ],
   "id": "14b2449bc1eee57a",
   "outputs": [],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:03.891606Z",
     "start_time": "2024-08-10T12:55:03.316364Z"
    }
   },
   "cell_type": "code",
   "source": "full_doc_retriever.add_documents(docs)",
   "id": "887e39b11c27197b",
   "outputs": [],
   "execution_count": 37
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:03.896428Z",
     "start_time": "2024-08-10T12:55:03.892693Z"
    }
   },
   "cell_type": "code",
   "source": "list(store.yield_keys())",
   "id": "352fc81716f73679",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['0865be28-e784-4128-86ad-5cb88166ee53']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 38
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:04.324536Z",
     "start_time": "2024-08-10T12:55:03.897439Z"
    }
   },
   "cell_type": "code",
   "source": "full_doc_retriever.add_documents(docs)",
   "id": "51a0b7db912073f6",
   "outputs": [],
   "execution_count": 39
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "1d00963333ade101"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:04.513989Z",
     "start_time": "2024-08-10T12:55:04.325761Z"
    }
   },
   "cell_type": "code",
   "source": [
    "op1 = vector_store.similarity_search(\"why customer called?\", k=2)\n",
    "op1"
   ],
   "id": "8104236c91a812f5",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'doc_id': '0865be28-e784-4128-86ad-5cb88166ee53', 'source': '/Users/mohitverma/Documents/Text_Question_Answer_RAG/text_data/sample_text.txt'}, page_content=\"customer: old mobile fee.\\nAgent: Oh, gotcha. So you already have those installed?\\ncustomer: Say that again, sir.\\nAgent: You already have those installed?\\ncustomer: Where?\\nAgent: No. I'm saying you you already have those installed?\\ncustomer: No. They they're in the process of doing it now.\\nAgent: Oh, gotcha. Okay. So, yeah, we'll we'll go ahead and get that\\nAgent: for the\"),\n",
       " Document(metadata={'doc_id': '64783626-7e04-4a6a-a441-fd14d1a4dd2f', 'source': '/Users/mohitverma/Documents/Text_Question_Answer_RAG/text_data/sample_text.txt'}, page_content=\"customer: old mobile fee.\\nAgent: Oh, gotcha. So you already have those installed?\\ncustomer: Say that again, sir.\\nAgent: You already have those installed?\\ncustomer: Where?\\nAgent: No. I'm saying you you already have those installed?\\ncustomer: No. They they're in the process of doing it now.\\nAgent: Oh, gotcha. Okay. So, yeah, we'll we'll go ahead and get that\\nAgent: for the\")]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:04.518544Z",
     "start_time": "2024-08-10T12:55:04.515278Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for i in op1:\n",
    "    print(i.page_content)\n",
    "    print('---------------------')"
   ],
   "id": "468f74321f5249d4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customer: old mobile fee.\n",
      "Agent: Oh, gotcha. So you already have those installed?\n",
      "customer: Say that again, sir.\n",
      "Agent: You already have those installed?\n",
      "customer: Where?\n",
      "Agent: No. I'm saying you you already have those installed?\n",
      "customer: No. They they're in the process of doing it now.\n",
      "Agent: Oh, gotcha. Okay. So, yeah, we'll we'll go ahead and get that\n",
      "Agent: for the\n",
      "---------------------\n",
      "customer: old mobile fee.\n",
      "Agent: Oh, gotcha. So you already have those installed?\n",
      "customer: Say that again, sir.\n",
      "Agent: You already have those installed?\n",
      "customer: Where?\n",
      "Agent: No. I'm saying you you already have those installed?\n",
      "customer: No. They they're in the process of doing it now.\n",
      "Agent: Oh, gotcha. Okay. So, yeah, we'll we'll go ahead and get that\n",
      "Agent: for the\n",
      "---------------------\n"
     ]
    }
   ],
   "execution_count": 41
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:04.621723Z",
     "start_time": "2024-08-10T12:55:04.519802Z"
    }
   },
   "cell_type": "code",
   "source": [
    "op2 = vector_store.similarity_search(\"what is customer name?\", k=2 )\n",
    "print(op2)"
   ],
   "id": "2ba1b8893f4fb47",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'doc_id': '64783626-7e04-4a6a-a441-fd14d1a4dd2f', 'source': '/Users/mohitverma/Documents/Text_Question_Answer_RAG/text_data/sample_text.txt'}, page_content='customer: Oh, okay. Okay.\\ncustomer: Okay. So yeah. Because I was wondering, you know, I just wanted to know what what was my current balance, you know, because\\ncustomer: Yeah.\\nAgent: So that So\\ncustomer: that we have the So that one nineteen\\ncustomer: is gonna be subtracted from the what the what the initial charge was originally supposed to be. Correct?\\nAgent: Yep.\\ncustomer: Which was one o the company'), Document(metadata={'doc_id': '64783626-7e04-4a6a-a441-fd14d1a4dd2f', 'source': '/Users/mohitverma/Documents/Text_Question_Answer_RAG/text_data/sample_text.txt'}, page_content='customer: Oh, okay. Okay.\\ncustomer: Okay. So yeah. Because I was wondering, you know, I just wanted to know what what was my current balance, you know, because\\ncustomer: Yeah.\\nAgent: So that So\\ncustomer: that we have the So that one nineteen\\ncustomer: is gonna be subtracted from the what the what the initial charge was originally supposed to be. Correct?\\nAgent: Yep.\\ncustomer: Which was one o the company')]\n"
     ]
    }
   ],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:04.729014Z",
     "start_time": "2024-08-10T12:55:04.622945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "op2 = vector_store.similarity_search(\"what is the agent name?\", k=2)\n",
    "print(op2)"
   ],
   "id": "453ab6ac3f28cf5d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Document(metadata={'doc_id': '0865be28-e784-4128-86ad-5cb88166ee53', 'source': '/Users/mohitverma/Documents/Text_Question_Answer_RAG/text_data/sample_text.txt'}, page_content=\"Agent: This is Martin with Goodyear.\\ncustomer: Hello. Is this Martin? Yeah. Hey, Martin. How are you doing? This is Dwayne Mann.\\nAgent: Hey, Dwayne.\\ncustomer: Hey. Yeah. The tires were here today.\\ncustomer: Is there any update on\\ncustomer: the\\ncustomer: because I haven't received the email about the the installation fee being waived and the return of the\\ncustomer: old mobile fee.\\nAgent: Oh, gotcha. So you already have those installed?\"), Document(metadata={'doc_id': '64783626-7e04-4a6a-a441-fd14d1a4dd2f', 'source': '/Users/mohitverma/Documents/Text_Question_Answer_RAG/text_data/sample_text.txt'}, page_content=\"Agent: This is Martin with Goodyear.\\ncustomer: Hello. Is this Martin? Yeah. Hey, Martin. How are you doing? This is Dwayne Mann.\\nAgent: Hey, Dwayne.\\ncustomer: Hey. Yeah. The tires were here today.\\ncustomer: Is there any update on\\ncustomer: the\\ncustomer: because I haven't received the email about the the installation fee being waived and the return of the\\ncustomer: old mobile fee.\\nAgent: Oh, gotcha. So you already have those installed?\")]\n"
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:04.733072Z",
     "start_time": "2024-08-10T12:55:04.730260Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "437c7ee491f2819c",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:04.736469Z",
     "start_time": "2024-08-10T12:55:04.734160Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "4267b0d4ee470d48",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:04.744559Z",
     "start_time": "2024-08-10T12:55:04.737382Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This text splitter is used to create the parent documents - The big chunks\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=600, chunk_overlap=chunk_overlap)\n",
    "\n",
    "# This text splitter is used to create the child documents - The small chunks\n",
    "# It should create documents smaller than the parent\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=300, chunk_overlap=100)\n",
    "\n",
    "# The vectorstore to use to index the child chunks\n",
    "vectorstore = Chroma(collection_name=\"split_parents\", embedding_function=bge_embeddings) #OpenAIEmbeddings()\n",
    "\n",
    "# The storage layer for the parent documents\n",
    "store = InMemoryStore()"
   ],
   "id": "e743a69b6d7e62a0",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:04.749310Z",
     "start_time": "2024-08-10T12:55:04.745552Z"
    }
   },
   "cell_type": "code",
   "source": [
    "big_chunks_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")"
   ],
   "id": "d8c1ffcc8a01240d",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:05.351808Z",
     "start_time": "2024-08-10T12:55:04.750786Z"
    }
   },
   "cell_type": "code",
   "source": "big_chunks_retriever.add_documents(docs)",
   "id": "833a90996c96df6a",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:05.356445Z",
     "start_time": "2024-08-10T12:55:05.352820Z"
    }
   },
   "cell_type": "code",
   "source": "len(list(store.yield_keys()))",
   "id": "352b51895245a9ed",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:05.385736Z",
     "start_time": "2024-08-10T12:55:05.357564Z"
    }
   },
   "cell_type": "code",
   "source": [
    "sub_docs1 = vectorstore.similarity_search(\"what is the customer name?\", k=3)\n",
    "\n",
    "for i in sub_docs1:\n",
    "    print(i.page_content)\n",
    "    print('---------------------')"
   ],
   "id": "bac794f8c64301dc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\"\"text = \"\"\"\n",
      "Agent: This is Martin with Goodyear.\n",
      "customer: Hello. Is this Martin? Yeah. Hey, Martin. How are you doing? This is Dwayne Mann.\n",
      "Agent: Hey, Dwayne.\n",
      "customer: Hey. Yeah. The tires were here today.\n",
      "customer: Is there any update on\n",
      "customer: the\n",
      "---------------------\n",
      "customer: Oh, okay. Okay.\n",
      "customer: Okay. So yeah. Because I was wondering, you know, I just wanted to know what what was my current balance, you know, because\n",
      "customer: Yeah.\n",
      "Agent: So that So\n",
      "customer: that we have the So that one nineteen\n",
      "---------------------\n",
      "customer: Oh, okay. Okay.\n",
      "customer: Okay. So yeah. Because I was wondering, you know, I just wanted to know what what was my current balance, you know, because\n",
      "customer: Yeah.\n",
      "Agent: So that So\n",
      "customer: that we have the So that one nineteen\n",
      "---------------------\n"
     ]
    }
   ],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:05.389445Z",
     "start_time": "2024-08-10T12:55:05.386777Z"
    }
   },
   "cell_type": "code",
   "source": "import os",
   "id": "fdda636865ba9a0",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:05.393369Z",
     "start_time": "2024-08-10T12:55:05.390360Z"
    }
   },
   "cell_type": "code",
   "source": "os.getenv(\"HUGGINGFACEHUB_API_TOKEN\", \"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\")",
   "id": "61eed245dda54e0c",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'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'"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 50
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "llm=HuggingFaceEndpoint(repo_id='mistralai/Mistral-7B-Instruct-v0.2',\n",
    "                        temperature=0.7, \n",
    "                        token=os.getenv(\"HUGGINGFACEHUB_API_TOKEN\"),\n",
    "                        max_length=512\n",
    "                        )\n"
   ],
   "id": "5befd3c7b2d72040"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:05.396334Z",
     "start_time": "2024-08-10T12:55:05.394289Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain.chains import LLMChain",
   "id": "b1f948b7e135c6c5",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:05.411487Z",
     "start_time": "2024-08-10T12:55:05.397288Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain_community.llms import HuggingFaceEndpoint",
   "id": "509f918a80d48d31",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:05.517779Z",
     "start_time": "2024-08-10T12:55:05.418467Z"
    }
   },
   "cell_type": "code",
   "source": [
    "repo_id = \"mistralai/Mistral-Nemo-Instruct-2407\"\n",
    "\n",
    "llm = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, temperature=0.7,\n",
    "    model_kwargs={'max_length': 1024, 'token':'hf_ViAIVdwUUpvshWmWITaaEPCbXtaeaqYe'},\n",
    "\n",
    ")"
   ],
   "id": "a481280fc9a35b08",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohitverma/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The class `HuggingFaceEndpoint` was deprecated in LangChain 0.0.37 and will be removed in 0.3. An updated version of the class exists in the langchain-huggingface package and should be used instead. To use it run `pip install -U langchain-huggingface` and import as `from langchain_huggingface import HuggingFaceEndpoint`.\n",
      "  warn_deprecated(\n"
     ]
    }
   ],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:05.522133Z",
     "start_time": "2024-08-10T12:55:05.518673Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=big_chunks_retriever)"
   ],
   "id": "228917dc612edde3",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:08.094399Z",
     "start_time": "2024-08-10T12:55:05.523448Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"what is the customer name?\"\n",
    "qa.run(query)"
   ],
   "id": "72cd9bcd75b50353",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohitverma/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:139: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 0.3.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" The customer's name is Dwayne Mann.\""
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 55
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:09.446029Z",
     "start_time": "2024-08-10T12:55:08.097098Z"
    }
   },
   "cell_type": "code",
   "source": [
    "query = \"what is the agent name?\"\n",
    "qa.run(query).strip()"
   ],
   "id": "d4970b34b00d7717",
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-Nemo-Instruct-2407 (Request ID: -qbwqHXd0fyUOAHUlOu1t)\n\nRate limit reached. Please log in or use a HF access token",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mHTTPError\u001B[0m                                 Traceback (most recent call last)",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:304\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     \u001B[43mresponse\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mraise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/requests/models.py:1024\u001B[0m, in \u001B[0;36mResponse.raise_for_status\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m   1023\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m http_error_msg:\n\u001B[0;32m-> 1024\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HTTPError(http_error_msg, response\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mself\u001B[39m)\n",
      "\u001B[0;31mHTTPError\u001B[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-Nemo-Instruct-2407",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[56], line 2\u001B[0m\n\u001B[1;32m      1\u001B[0m query \u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwhat is the agent name?\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m----> 2\u001B[0m \u001B[43mqa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mquery\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mstrip()\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    167\u001B[0m     emit_warning()\n\u001B[0;32m--> 168\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/base.py:600\u001B[0m, in \u001B[0;36mChain.run\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    598\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(args) \u001B[38;5;241m!=\u001B[39m \u001B[38;5;241m1\u001B[39m:\n\u001B[1;32m    599\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supports only one positional argument.\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m--> 600\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43margs\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[1;32m    601\u001B[0m         _output_key\n\u001B[1;32m    602\u001B[0m     ]\n\u001B[1;32m    604\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[1;32m    605\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(kwargs, callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[1;32m    606\u001B[0m         _output_key\n\u001B[1;32m    607\u001B[0m     ]\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    167\u001B[0m     emit_warning()\n\u001B[0;32m--> 168\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/base.py:383\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \n\u001B[1;32m    353\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    376\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[1;32m    381\u001B[0m }\n\u001B[0;32m--> 383\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/base.py:166\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    165\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    167\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/base.py:156\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    155\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 156\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    159\u001B[0m     )\n\u001B[1;32m    161\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    162\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[1;32m    163\u001B[0m     )\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/retrieval_qa/base.py:146\u001B[0m, in \u001B[0;36mBaseRetrievalQA._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    144\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    145\u001B[0m     docs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_docs(question)  \u001B[38;5;66;03m# type: ignore[call-arg]\u001B[39;00m\n\u001B[0;32m--> 146\u001B[0m answer \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcombine_documents_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mrun\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    147\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_documents\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mquestion\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mquestion\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_run_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    148\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    150\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mreturn_source_documents:\n\u001B[1;32m    151\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m {\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key: answer, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msource_documents\u001B[39m\u001B[38;5;124m\"\u001B[39m: docs}\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    167\u001B[0m     emit_warning()\n\u001B[0;32m--> 168\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/base.py:605\u001B[0m, in \u001B[0;36mChain.run\u001B[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001B[0m\n\u001B[1;32m    600\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m(args[\u001B[38;5;241m0\u001B[39m], callbacks\u001B[38;5;241m=\u001B[39mcallbacks, tags\u001B[38;5;241m=\u001B[39mtags, metadata\u001B[38;5;241m=\u001B[39mmetadata)[\n\u001B[1;32m    601\u001B[0m         _output_key\n\u001B[1;32m    602\u001B[0m     ]\n\u001B[1;32m    604\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[0;32m--> 605\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mtags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m)\u001B[49m[\n\u001B[1;32m    606\u001B[0m         _output_key\n\u001B[1;32m    607\u001B[0m     ]\n\u001B[1;32m    609\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m kwargs \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m args:\n\u001B[1;32m    610\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    611\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m`run` supported with either positional arguments or keyword arguments,\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    612\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but none were provided.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    613\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    167\u001B[0m     emit_warning()\n\u001B[0;32m--> 168\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/base.py:383\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \n\u001B[1;32m    353\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    376\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[1;32m    381\u001B[0m }\n\u001B[0;32m--> 383\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/base.py:166\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    165\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    167\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/base.py:156\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    155\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 156\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    159\u001B[0m     )\n\u001B[1;32m    161\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    162\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[1;32m    163\u001B[0m     )\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/combine_documents/base.py:138\u001B[0m, in \u001B[0;36mBaseCombineDocumentsChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    136\u001B[0m \u001B[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001B[39;00m\n\u001B[1;32m    137\u001B[0m other_keys \u001B[38;5;241m=\u001B[39m {k: v \u001B[38;5;28;01mfor\u001B[39;00m k, v \u001B[38;5;129;01min\u001B[39;00m inputs\u001B[38;5;241m.\u001B[39mitems() \u001B[38;5;28;01mif\u001B[39;00m k \u001B[38;5;241m!=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minput_key}\n\u001B[0;32m--> 138\u001B[0m output, extra_return_dict \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcombine_docs\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    139\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdocs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m_run_manager\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mget_child\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mother_keys\u001B[49m\n\u001B[1;32m    140\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    141\u001B[0m extra_return_dict[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key] \u001B[38;5;241m=\u001B[39m output\n\u001B[1;32m    142\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m extra_return_dict\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/combine_documents/stuff.py:249\u001B[0m, in \u001B[0;36mStuffDocumentsChain.combine_docs\u001B[0;34m(self, docs, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    247\u001B[0m inputs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_get_inputs(docs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m    248\u001B[0m \u001B[38;5;66;03m# Call predict on the LLM.\u001B[39;00m\n\u001B[0;32m--> 249\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_chain\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpredict\u001B[49m\u001B[43m(\u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43minputs\u001B[49m\u001B[43m)\u001B[49m, {}\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/llm.py:318\u001B[0m, in \u001B[0;36mLLMChain.predict\u001B[0;34m(self, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mpredict\u001B[39m(\u001B[38;5;28mself\u001B[39m, callbacks: Callbacks \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m \u001B[38;5;28mstr\u001B[39m:\n\u001B[1;32m    304\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Format prompt with kwargs and pass to LLM.\u001B[39;00m\n\u001B[1;32m    305\u001B[0m \n\u001B[1;32m    306\u001B[0m \u001B[38;5;124;03m    Args:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    316\u001B[0m \u001B[38;5;124;03m            completion = llm.predict(adjective=\"funny\")\u001B[39;00m\n\u001B[1;32m    317\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[0;32m--> 318\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m)\u001B[49m[\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39moutput_key]\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/_api/deprecation.py:168\u001B[0m, in \u001B[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001B[0;34m(*args, **kwargs)\u001B[0m\n\u001B[1;32m    166\u001B[0m     warned \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[1;32m    167\u001B[0m     emit_warning()\n\u001B[0;32m--> 168\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mwrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/base.py:383\u001B[0m, in \u001B[0;36mChain.__call__\u001B[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001B[0m\n\u001B[1;32m    351\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Execute the chain.\u001B[39;00m\n\u001B[1;32m    352\u001B[0m \n\u001B[1;32m    353\u001B[0m \u001B[38;5;124;03mArgs:\u001B[39;00m\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    374\u001B[0m \u001B[38;5;124;03m        `Chain.output_keys`.\u001B[39;00m\n\u001B[1;32m    375\u001B[0m \u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[1;32m    376\u001B[0m config \u001B[38;5;241m=\u001B[39m {\n\u001B[1;32m    377\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks,\n\u001B[1;32m    378\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtags\u001B[39m\u001B[38;5;124m\"\u001B[39m: tags,\n\u001B[1;32m    379\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mmetadata\u001B[39m\u001B[38;5;124m\"\u001B[39m: metadata,\n\u001B[1;32m    380\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_name\u001B[39m\u001B[38;5;124m\"\u001B[39m: run_name,\n\u001B[1;32m    381\u001B[0m }\n\u001B[0;32m--> 383\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minvoke\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    384\u001B[0m \u001B[43m    \u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    385\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcast\u001B[49m\u001B[43m(\u001B[49m\u001B[43mRunnableConfig\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m{\u001B[49m\u001B[43mk\u001B[49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mconfig\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mitems\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mv\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mnot\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m}\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    386\u001B[0m \u001B[43m    \u001B[49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturn_only_outputs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    387\u001B[0m \u001B[43m    \u001B[49m\u001B[43minclude_run_info\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minclude_run_info\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    388\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/base.py:166\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    165\u001B[0m     run_manager\u001B[38;5;241m.\u001B[39mon_chain_error(e)\n\u001B[0;32m--> 166\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    167\u001B[0m run_manager\u001B[38;5;241m.\u001B[39mon_chain_end(outputs)\n\u001B[1;32m    169\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m include_run_info:\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/base.py:156\u001B[0m, in \u001B[0;36mChain.invoke\u001B[0;34m(self, input, config, **kwargs)\u001B[0m\n\u001B[1;32m    153\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    154\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_inputs(inputs)\n\u001B[1;32m    155\u001B[0m     outputs \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 156\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    157\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(inputs)\n\u001B[1;32m    159\u001B[0m     )\n\u001B[1;32m    161\u001B[0m     final_outputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprep_outputs(\n\u001B[1;32m    162\u001B[0m         inputs, outputs, return_only_outputs\n\u001B[1;32m    163\u001B[0m     )\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/llm.py:128\u001B[0m, in \u001B[0;36mLLMChain._call\u001B[0;34m(self, inputs, run_manager)\u001B[0m\n\u001B[1;32m    123\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_call\u001B[39m(\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    125\u001B[0m     inputs: Dict[\u001B[38;5;28mstr\u001B[39m, Any],\n\u001B[1;32m    126\u001B[0m     run_manager: Optional[CallbackManagerForChainRun] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m    127\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m Dict[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mstr\u001B[39m]:\n\u001B[0;32m--> 128\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m[\u001B[49m\u001B[43minputs\u001B[49m\u001B[43m]\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    129\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_outputs(response)[\u001B[38;5;241m0\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain/chains/llm.py:140\u001B[0m, in \u001B[0;36mLLMChain.generate\u001B[0;34m(self, input_list, run_manager)\u001B[0m\n\u001B[1;32m    138\u001B[0m callbacks \u001B[38;5;241m=\u001B[39m run_manager\u001B[38;5;241m.\u001B[39mget_child() \u001B[38;5;28;01mif\u001B[39;00m run_manager \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm, BaseLanguageModel):\n\u001B[0;32m--> 140\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate_prompt\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    141\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    142\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    143\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    144\u001B[0m \u001B[43m        \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mllm_kwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    145\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    146\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    147\u001B[0m     results \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm\u001B[38;5;241m.\u001B[39mbind(stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mllm_kwargs)\u001B[38;5;241m.\u001B[39mbatch(\n\u001B[1;32m    148\u001B[0m         cast(List, prompts), {\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcallbacks\u001B[39m\u001B[38;5;124m\"\u001B[39m: callbacks}\n\u001B[1;32m    149\u001B[0m     )\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/language_models/llms.py:703\u001B[0m, in \u001B[0;36mBaseLLM.generate_prompt\u001B[0;34m(self, prompts, stop, callbacks, **kwargs)\u001B[0m\n\u001B[1;32m    695\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mgenerate_prompt\u001B[39m(\n\u001B[1;32m    696\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    697\u001B[0m     prompts: List[PromptValue],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    700\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    701\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    702\u001B[0m     prompt_strings \u001B[38;5;241m=\u001B[39m [p\u001B[38;5;241m.\u001B[39mto_string() \u001B[38;5;28;01mfor\u001B[39;00m p \u001B[38;5;129;01min\u001B[39;00m prompts]\n\u001B[0;32m--> 703\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mgenerate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt_strings\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/language_models/llms.py:882\u001B[0m, in \u001B[0;36mBaseLLM.generate\u001B[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001B[0m\n\u001B[1;32m    867\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m get_llm_cache() \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcache \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mFalse\u001B[39;00m:\n\u001B[1;32m    868\u001B[0m     run_managers \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    869\u001B[0m         callback_manager\u001B[38;5;241m.\u001B[39mon_llm_start(\n\u001B[1;32m    870\u001B[0m             dumpd(\u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    880\u001B[0m         )\n\u001B[1;32m    881\u001B[0m     ]\n\u001B[0;32m--> 882\u001B[0m     output \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate_helper\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    883\u001B[0m \u001B[43m        \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43mbool\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mnew_arg_supported\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\n\u001B[1;32m    884\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    885\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m output\n\u001B[1;32m    886\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(missing_prompts) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/language_models/llms.py:740\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    738\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n\u001B[1;32m    739\u001B[0m         run_manager\u001B[38;5;241m.\u001B[39mon_llm_error(e, response\u001B[38;5;241m=\u001B[39mLLMResult(generations\u001B[38;5;241m=\u001B[39m[]))\n\u001B[0;32m--> 740\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n\u001B[1;32m    741\u001B[0m flattened_outputs \u001B[38;5;241m=\u001B[39m output\u001B[38;5;241m.\u001B[39mflatten()\n\u001B[1;32m    742\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m manager, flattened_output \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(run_managers, flattened_outputs):\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/language_models/llms.py:727\u001B[0m, in \u001B[0;36mBaseLLM._generate_helper\u001B[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001B[0m\n\u001B[1;32m    717\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_generate_helper\u001B[39m(\n\u001B[1;32m    718\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m    719\u001B[0m     prompts: List[\u001B[38;5;28mstr\u001B[39m],\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    723\u001B[0m     \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs: Any,\n\u001B[1;32m    724\u001B[0m ) \u001B[38;5;241m-\u001B[39m\u001B[38;5;241m>\u001B[39m LLMResult:\n\u001B[1;32m    725\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    726\u001B[0m         output \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m--> 727\u001B[0m             \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_generate\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    728\u001B[0m \u001B[43m                \u001B[49m\u001B[43mprompts\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    729\u001B[0m \u001B[43m                \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    730\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;66;43;03m# TODO: support multiple run managers\u001B[39;49;00m\n\u001B[1;32m    731\u001B[0m \u001B[43m                \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_managers\u001B[49m\u001B[43m[\u001B[49m\u001B[38;5;241;43m0\u001B[39;49m\u001B[43m]\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mrun_managers\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    732\u001B[0m \u001B[43m                \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    733\u001B[0m \u001B[43m            \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    734\u001B[0m             \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m    735\u001B[0m             \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_generate(prompts, stop\u001B[38;5;241m=\u001B[39mstop)\n\u001B[1;32m    736\u001B[0m         )\n\u001B[1;32m    737\u001B[0m     \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mBaseException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    738\u001B[0m         \u001B[38;5;28;01mfor\u001B[39;00m run_manager \u001B[38;5;129;01min\u001B[39;00m run_managers:\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_core/language_models/llms.py:1431\u001B[0m, in \u001B[0;36mLLM._generate\u001B[0;34m(self, prompts, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m   1428\u001B[0m new_arg_supported \u001B[38;5;241m=\u001B[39m inspect\u001B[38;5;241m.\u001B[39msignature(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call)\u001B[38;5;241m.\u001B[39mparameters\u001B[38;5;241m.\u001B[39mget(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrun_manager\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m   1429\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m prompt \u001B[38;5;129;01min\u001B[39;00m prompts:\n\u001B[1;32m   1430\u001B[0m     text \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m-> 1431\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mstop\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mstop\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mrun_manager\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mrun_manager\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   1432\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m new_arg_supported\n\u001B[1;32m   1433\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_call(prompt, stop\u001B[38;5;241m=\u001B[39mstop, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m   1434\u001B[0m     )\n\u001B[1;32m   1435\u001B[0m     generations\u001B[38;5;241m.\u001B[39mappend([Generation(text\u001B[38;5;241m=\u001B[39mtext)])\n\u001B[1;32m   1436\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m LLMResult(generations\u001B[38;5;241m=\u001B[39mgenerations)\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/langchain_community/llms/huggingface_endpoint.py:267\u001B[0m, in \u001B[0;36mHuggingFaceEndpoint._call\u001B[0;34m(self, prompt, stop, run_manager, **kwargs)\u001B[0m\n\u001B[1;32m    263\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    264\u001B[0m     invocation_params[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop\u001B[39m\u001B[38;5;124m\"\u001B[39m] \u001B[38;5;241m=\u001B[39m invocation_params[\n\u001B[1;32m    265\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstop_sequences\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    266\u001B[0m     ]  \u001B[38;5;66;03m# porting 'stop_sequences' into the 'stop' argument\u001B[39;00m\n\u001B[0;32m--> 267\u001B[0m     response \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mclient\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mpost\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m    268\u001B[0m \u001B[43m        \u001B[49m\u001B[43mjson\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m{\u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43minputs\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43mprompt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mparameters\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m:\u001B[49m\u001B[43m \u001B[49m\u001B[43minvocation_params\u001B[49m\u001B[43m}\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    269\u001B[0m \u001B[43m        \u001B[49m\u001B[43mstream\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m    270\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtask\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mtask\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m    271\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    272\u001B[0m     \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    273\u001B[0m         response_text \u001B[38;5;241m=\u001B[39m json\u001B[38;5;241m.\u001B[39mloads(response\u001B[38;5;241m.\u001B[39mdecode())[\u001B[38;5;241m0\u001B[39m][\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mgenerated_text\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/inference/_client.py:304\u001B[0m, in \u001B[0;36mInferenceClient.post\u001B[0;34m(self, json, data, model, task, stream)\u001B[0m\n\u001B[1;32m    301\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m InferenceTimeoutError(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mInference call timed out: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00murl\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01merror\u001B[39;00m  \u001B[38;5;66;03m# type: ignore\u001B[39;00m\n\u001B[1;32m    303\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 304\u001B[0m     \u001B[43mhf_raise_for_status\u001B[49m\u001B[43m(\u001B[49m\u001B[43mresponse\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    305\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m response\u001B[38;5;241m.\u001B[39miter_lines() \u001B[38;5;28;01mif\u001B[39;00m stream \u001B[38;5;28;01melse\u001B[39;00m response\u001B[38;5;241m.\u001B[39mcontent\n\u001B[1;32m    306\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m HTTPError \u001B[38;5;28;01mas\u001B[39;00m error:\n",
      "File \u001B[0;32m~/miniconda3/envs/tensorflow/lib/python3.10/site-packages/huggingface_hub/utils/_errors.py:371\u001B[0m, in \u001B[0;36mhf_raise_for_status\u001B[0;34m(response, endpoint_name)\u001B[0m\n\u001B[1;32m    367\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m HfHubHTTPError(message, response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n\u001B[1;32m    369\u001B[0m \u001B[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001B[39;00m\n\u001B[1;32m    370\u001B[0m \u001B[38;5;66;03m# as well (request id and/or server error message)\u001B[39;00m\n\u001B[0;32m--> 371\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m HfHubHTTPError(\u001B[38;5;28mstr\u001B[39m(e), response\u001B[38;5;241m=\u001B[39mresponse) \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01me\u001B[39;00m\n",
      "\u001B[0;31mHfHubHTTPError\u001B[0m: 429 Client Error: Too Many Requests for url: https://api-inference.huggingface.co/models/mistralai/Mistral-Nemo-Instruct-2407 (Request ID: -qbwqHXd0fyUOAHUlOu1t)\n\nRate limit reached. Please log in or use a HF access token"
     ]
    }
   ],
   "execution_count": 56
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:55:09.447351Z",
     "start_time": "2024-08-10T12:55:09.447275Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain.llms import CTransformers",
   "id": "23a61dc5736609b8",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "repo_id = \"TheBloke/NeuralHermes-2.5-Mistral-7B-GPTQ\"\n",
    "\n",
    "llm2 = HuggingFaceEndpoint(\n",
    "    repo_id=repo_id, temperature=0.7,\n",
    "    model_kwargs={'max_length': 1024, 'token':'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'},\n",
    "\n",
    ")"
   ],
   "id": "3fd14a06f7a0a5da",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain_community.llms.huggingface_pipeline import HuggingFacePipeline\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "\n",
    "model_id = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id).to('mps')\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=1024)\n",
    "hf = HuggingFacePipeline(pipeline=pipe)"
   ],
   "id": "98e23caa1505656e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=hf,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=big_chunks_retriever)"
   ],
   "id": "209b533dca1bf5ec",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = \"what is the customer name?\"\n",
    "qa.run(query)"
   ],
   "id": "619de43e0bfdcfb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "query = \"what is the agent name?\"\n",
    "qa.run(query).strip()"
   ],
   "id": "ae1723f189551636",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "from langchain.llms import CTransformers",
   "id": "95956d144af14961",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": "!wget \"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGML/blob/main/llama-2-7b-chat.ggmlv3.q2_K.bin\"",
   "id": "8b50878aa6b7d941"
  },
  {
   "metadata": {},
   "cell_type": "raw",
   "source": [
    "llm = CTransformers(model=\"/Users/mohitverma/Downloads/llama-2-7b-chat.ggmlv3.q2_K.bin\",\n",
    "                        model_type='llama',\n",
    "                        config={'max_new_tokens': 1024,\n",
    "                                'temperature': 0.7},\n",
    "                        device='mps'\n",
    "                        )"
   ],
   "id": "a73b0e38d4bb7653"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "#llm.invoke('ed')",
   "id": "afe2e3c007dd6ece",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "import os",
   "id": "c9d8046c8150c306",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "os.getenv(\"AI21_API_KEY\", \"KVlTLRfckUbPN81UiTC6OPtO1tjzLbC6\")",
   "id": "de72d72e1d312c98",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:57:41.173310Z",
     "start_time": "2024-08-10T12:57:41.165234Z"
    }
   },
   "cell_type": "code",
   "source": "from langchain_ai21 import AI21LLM, AI21Embeddings, ChatAI21",
   "id": "ad9342b5339c023d",
   "outputs": [],
   "execution_count": 57
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:58:34.598103Z",
     "start_time": "2024-08-10T12:58:34.529453Z"
    }
   },
   "cell_type": "code",
   "source": "llm4=ChatAI21(model='jamba-instruct',temperature=0.0, max_tokens=4096,api_key='cVrHnlYnqsx1SUIw8jijyiqy07vBNuSY')",
   "id": "681ca28570548045",
   "outputs": [],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:58:35.133569Z",
     "start_time": "2024-08-10T12:58:35.129330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(llm=llm4,\n",
    "                                 chain_type=\"stuff\",\n",
    "                                 retriever=big_chunks_retriever)"
   ],
   "id": "701e58c262816780",
   "outputs": [],
   "execution_count": 62
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:58:37.570247Z",
     "start_time": "2024-08-10T12:58:35.934293Z"
    }
   },
   "cell_type": "code",
   "source": "qa.run('what is the customer name?')",
   "id": "962f68ee95e8d2c6",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dwayne Mann.'"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 63
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T12:58:46.928765Z",
     "start_time": "2024-08-10T12:58:45.542745Z"
    }
   },
   "cell_type": "code",
   "source": "qa.run('what is the agent name?')",
   "id": "e0c94114b4f31d3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Martin.'"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 64
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T13:01:20.010103Z",
     "start_time": "2024-08-10T13:01:17.671208Z"
    }
   },
   "cell_type": "code",
   "source": "qa.run('why customer called?')",
   "id": "ca4d14eeecfb90f0",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The customer called to inquire about the status of their tire installation and to confirm that the installation fee would be waived and the old mobile fee would be returned, as they had not received an email confirmation for these arrangements.'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T13:01:51.762623Z",
     "start_time": "2024-08-10T13:01:48.340099Z"
    }
   },
   "cell_type": "code",
   "source": "qa.run('give me the summary of the conversation')",
   "id": "4410b7975a5f0af3",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Dwayne Mann called Goodyear to inquire about the status of his tire installation and the waiver of the installation fee and the return of the old mobile fee. He confirmed that the tires were delivered today but was unsure about the installation fee waiver and the return of the old mobile fee. Martin, the representative from Goodyear, confirmed that Dwayne already has the tires installed and asked if he received the email regarding the installation fee waiver and the return of the old mobile fee.'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 66
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T13:06:51.847702Z",
     "start_time": "2024-08-10T13:06:48.438658Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(qa.run(\"\"\"\n",
    "You are an expert at analyzing conversation transcripts. \n",
    "        Please analyze the transcribed texts provided by user, and work out the following:\n",
    "\n",
    "        Q1>. Summarize transcription in two short sentences:\n",
    "        Q2>. Rate sentiment of the transcription between 0 and 100, answer in number:\n",
    "        Q3>. Extract key entities from transcription:\n",
    "        Q4>. Which category best describes the transcription [monologue, enquiry, news, chitchat, complain, complement, other]:\n",
    "\"\"\"))"
   ],
   "id": "2f3d859ecbf42a36",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Q1: Dwayne Mann contacts Martin from Goodyear to inquire about the status of his tire installation and the waiver of the installation fee and old mobile fee. Martin confirms the installation and discusses the rebate card balance.\\n\\nQ2: 80.\\n\\nQ3: Dwayne Mann, Martin, Goodyear, tires, installation, fee, rebate card, balance.\\n\\nQ4: enquiry.'"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 67
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T13:07:10.934462Z",
     "start_time": "2024-08-10T13:07:08.253046Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(qa.run(\"\"\"\n",
    "You are an expert at analyzing conversation transcripts. \n",
    "        Please analyze the transcribed texts provided by user, and work out the following:\n",
    "\n",
    "        Q1>. Summarize transcription in two short sentences:\n",
    "        Q2>. Rate sentiment of the transcription between 0 and 100, answer in number:\n",
    "        Q3>. Extract key entities from transcription also their tag:\n",
    "        Q4>. Which category best describes the transcription [monologue, enquiry, news, chitchat, complain, complement, other]:\n",
    "\"\"\"))"
   ],
   "id": "e0783b0956b963f1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Dwayne Mann contacts Martin from Goodyear to inquire about the status of his tire installation and the waiver of the installation fee and old mobile fee. Martin confirms the installation and discusses the rebate card balance.\n",
      "\n",
      "Q2: 80.\n",
      "\n",
      "Q3: Dwayne Mann, Martin, Goodyear, tires, installation, fee, rebate card, balance.\n",
      "\n",
      "Q4: enquiry.\n"
     ]
    }
   ],
   "execution_count": 68
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T13:07:36.637884Z",
     "start_time": "2024-08-10T13:07:33.850175Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(qa.run(\"\"\"\n",
    "You are an expert at analyzing conversation transcripts. \n",
    "        Please analyze the transcribed texts provided by user, and work out the following:\n",
    "\n",
    "        Q1>. Summarize transcription in two short sentences:\n",
    "        Q2>. Rate sentiment of the transcription between 0 and 100, answer in number:\n",
    "        Q3>. Extract key entities from transcription also their tag:\n",
    "        Q4>. Which category best describes the transcription [monologue, enquiry, news, chitchat, complain, complement, other]:\n",
    "\"\"\"))"
   ],
   "id": "14b5b5e46a6a7629",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Dwayne Mann calls Martin from Goodyear to inquire about the status of his tire installation and the waiver of the installation fee and old mobile fee.\n",
      "Q2: 80.\n",
      "Q3: Dwayne Mann (customer), Martin (agent), Goodyear (company).\n",
      "Q4: enquiry.\n"
     ]
    }
   ],
   "execution_count": 69
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T13:11:12.194712Z",
     "start_time": "2024-08-10T13:11:06.925717Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(qa.run(\"\"\"\n",
    "             You are an expert at analyzing conversation transcripts. Please analyze the transcribed texts provided by the user, and provide the following insights:\n",
    "\n",
    "Q1>. Summarize the transcription in two concise sentences.\n",
    "Q2>. Rate the sentiment of the transcription on a scale from 0 (very negative) to 100 (very positive). Answer in a single number.\n",
    "Q3>. Identify and list the key entities mentioned in the transcription.\n",
    "Q4>. Categorize the transcription by selecting the most appropriate category from the following options: [monologue, enquiry, news, chitchat, complaint, compliment, other].\n",
    "Q5>. Highlight the main intent of the conversation in one sentence.\n",
    "Q6>. Detect any specific requests or actions required from the conversation.\n",
    "Q7>. Identify the tone of the conversation [formal, informal, neutral, aggressive, empathetic, frustrated, other].\n",
    "Q8>. Extract any potential follow-up actions or next steps suggested by the transcription.\n",
    "Q9>. Determine the duration of the conversation based on the provided text (if applicable).\n",
    "Q10>. Identify any patterns or recurring themes in the transcription that might indicate underlying issues or trends.\"\"\"))"
   ],
   "id": "ad9f06b94aad44b3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: Dwayne Mann contacts Martin from Goodyear to inquire about the status of his tire installation and rebate, expressing concern about the installation fee and old mobile fee not being waived.\n",
      "\n",
      "Q2: 80.\n",
      "\n",
      "Q3: Dwayne Mann, Martin (Goodyear), bank, tires, installation fee, old mobile fee, rebate card.\n",
      "\n",
      "Q4: enquiry.\n",
      "\n",
      "Q5: Dwayne Mann is seeking an update on the status of his tire installation and the resolution of the installation fee and old mobile fee.\n",
      "\n",
      "Q6: Dwayne Mann requests an update on the tire installation and the resolution of the installation fee and old mobile fee.\n",
      "\n",
      "Q7: Informal.\n",
      "\n",
      "Q8: Dwayne Mann is advised to reach out to Martin once the installation is completed for further assistance.\n",
      "\n",
      "Q9: The duration of the conversation is not explicitly mentioned in the provided text.\n",
      "\n",
      "Q10: The recurring theme in the transcription is the concern about the installation fee and old mobile fee not being waived, as well as the status of the tire installation.\n"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-10T13:29:44.663498Z",
     "start_time": "2024-08-10T13:29:36.250109Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(qa.run(\"\"\"\n",
    "You are an expert at analyzing conversation transcripts. Please analyze the transcribed texts provided by the user, and provide the following insights:\n",
    "\n",
    "Q1>. Summarize the transcription in two concise sentences.\n",
    "Q2>. Rate the sentiment of the transcription on a scale from 0 (very negative) to 100 (very positive). Answer in a single number.\n",
    "Q3>. Identify and list the key entities mentioned in the transcription.\n",
    "Q4>. Categorize the transcription by selecting the most appropriate category from the following options: [monologue, enquiry, news, chitchat, complaint, compliment, other].\n",
    "Q5>. Highlight the main intent of the conversation in one sentence.\n",
    "Q6>. Detect any specific requests or actions required from the conversation.\n",
    "Q7>. Identify the tone of the conversation [formal, informal, neutral, aggressive, empathetic, frustrated, other].\n",
    "Q8>. Extract any potential follow-up actions or next steps suggested by the transcription.\n",
    "Q9>. Determine the duration of the conversation based on the provided text (if applicable).\n",
    "Q10>. Identify any patterns or recurring themes in the transcription that might indicate underlying issues or trends.\n",
    "Q11>. Pinpoint any technical or product-related issues mentioned in the conversation.\n",
    "Q12>. Detect and list any keywords or phrases that are repeated frequently throughout the conversation.\n",
    "Q13>. Assess the overall satisfaction level of the customer based on the transcription, on a scale from 0 (very unsatisfied) to 100 (very satisfied).\n",
    "Q14>. Identify if any escalation is suggested or implied in the conversation.\n",
    "Q15>. Determine if the conversation involves any sensitive information or data that requires special attention (e.g., personal details, financial information).\n",
    "Q16>. Recognize if the conversation mentions any competitors or alternative products/services.\n",
    "Q17>. Detect any discrepancies or contradictions in the conversation that might require clarification.\n",
    "Q18>. Identify the customers primary concerns or priorities mentioned in the conversation.\n",
    "Q19>. Determine if the conversation outcome was resolved or if it remains unresolved.\n",
    "Q20>. Provide any additional observations or insights that might be relevant to understanding the overall context of the conversation, if Not say NA.\n",
    "\"\"\"))"
   ],
   "id": "7747e737ca2914b7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q1: The transcription involves a conversation between Dwayne Mann and Martin from Goodyear, discussing the installation of tires and the status of a rebate card.\n",
      "Q2: The sentiment of the transcription is 80, indicating a generally positive tone.\n",
      "Q3: Key entities mentioned in the transcription include Dwayne Mann, Martin, Goodyear, bank, tires, rebate card, installation fee, old mobile fee.\n",
      "Q4: The transcription can be categorized as a chitchat, as it involves a casual conversation between Dwayne Mann and Martin.\n",
      "Q5: The main intent of the conversation is to inquire about the status of the rebate card and the installation fee.\n",
      "Q6: The specific request from Dwayne Mann is to confirm if the rebate card can be used towards the balance and if the installation fee and old mobile fee will be waived.\n",
      "Q7: The tone of the conversation is informal, as it is a casual and friendly exchange between Dwayne Mann and Martin.\n",
      "Q8: The potential follow-up actions suggested by the transcription include Dwayne Mann reaching out to Martin once the installation is completed, and Martin reaching out to the bank to confirm the credit back.\n",
      "Q9: The duration of the conversation is not explicitly mentioned in the provided text.\n",
      "Q10: There are no recurring themes or patterns in the transcription that indicate underlying issues or trends.\n",
      "Q11: There are no technical or product-related issues mentioned in the conversation.\n",
      "Q12: Keywords or phrases that are frequently mentioned in the conversation include \"rebate card,\" \"installation fee,\" \"old mobile fee,\" \"bank,\" \"tires.\"\n",
      "Q13: The overall satisfaction level of the customer is 80, indicating a generally positive level of satisfaction.\n",
      "Q14: There is no indication of any escalation in the conversation.\n",
      "Q15: There is no sensitive information or data mentioned in the conversation that requires special attention.\n",
      "Q16: There are no mentions of competitors or alternative products/services in the conversation.\n",
      "Q17: There are no discrepancies or contradictions in the conversation that require clarification.\n",
      "Q18: The customer's primary concerns or priorities mentioned in the conversation are the status of the rebate card and the installation fee.\n",
      "Q19: The conversation outcome is not explicitly mentioned, so it is unclear if it was resolved or remains unresolved.\n",
      "Q20: NA.\n"
     ]
    }
   ],
   "execution_count": 72
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "9a105b3f586b5e75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "853b5820983add7f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "864bf6b2cbbd75c6"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
